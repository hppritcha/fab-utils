
Instructions for building and running DOE mini app LULESH.
For additional information see internal Mini Apps wiki.

1. Download from
   https://codesign.llnl.gov/lulesh.php

2. A top level README mostly describes recent changes and does not provide build or run 
   information.

3. Load necessary PrgEnv module, set PATH and LD_LIBRARY_PATH and set recommended
   environment variables, e.g. for OMPI, etc.

4. Modify Makefile. a) is necessary, b) depends on compiler used. 

   a) Set the compiler name
   #MPICXX = mpig++ -DUSE_MPI=1
   MPICXX = mpic++ -DUSE_MPI=1

   b) When using Intel and building for KNL, add KNL option and optionally -qopt-report if an
      optimization report should be generated.
   #CXXFLAGS = -g -O3 -fopenmp -I. -Wall
   CXXFLAGS = -g -O3 -fopenmp -I. -Wall -axMIC-AVX512

5. > make clean
   > make
   This creates the executable lulesh2.0 in the same directory.

6. Now build other apps and run the app run script run_minapps found in the same directory
   as these notes. For run instructions see comments in the script itself.

   Alternatively, a few suggested launch options follow. For each example, the number
   of nodes used can be varied depending on whether XEON or KNL nodes are used. A requirement
   when running LULESH is that the number of MPI tasks must always be the cube of an integer.
   If this requirement is not satisfied, LULESH will just abort but not provide a hint as to
   what went wrong. LULESH is a strong scaling problem and run times go up with the number
   of ranks or threads. The option -i can be used to limit number of iterations and in turn
   reduce run time. LULESH does not produce output while it is running, only the results
   at the end. Be patient.

   a )To run a small problem across 2 nodes:
   > export OMP_NUM_THREADS=4
   > srun -n 8 -N 2 -c 4 --exclusive <path/exec> -s 38
   or
   > export OMP_NUM_THREADS=4
   > aprun -n 8 -N 4 -d 4 <path/exec>  -s 38 

   where <path/exec> points to the binary.

   b) To run on 16 small nodes:
   > export OMP_NUM_THREADS=4
   > srun -n 64 --ntasks-per-node=4 -c 4 --exclusive <path/exec> -s 38 -i 100
   or
   > export OMP_NUM_THREADS=4
   > aprun -n 64 -N 4 -d 4 -cc depth <path/exec> -s 38

   c) To run a medium size job across however many nodes:
   > export OMP_NUM_THREADS=2
   > srun -n 216 -c 2 --exclusive <path/exec> -s 38 -i 100 
   or 
   > export OMP_NUM_THREADS=2
   > aprun -n 216 -d 2 -cc depth <path/exec> -s 48 -i 200

   e) Maximum size run on current KNL systems, SLURM only
   > export OMP_NUM_THREADS=4
   > srun -n 8000 -N 125 -c 4 --threads-per-core 4 --hint=multithread --exclusive <path/exec> -s 48 -i 10

7. For each run, look for the following lines at the end of the output

   Elapsed time         =      ..... (s)
   Grind time (us/z/c)  =  ......... (per dom)  (........... overall)
   FOM                  =  ......... (z/s)

8. The Figure of Merit (FOM) in the last line reports elements solved per microsecond
   and is the most relevant performance data.

   None of the srun or aprun command line options should be assumed to be optimal for
   performance investigations.

